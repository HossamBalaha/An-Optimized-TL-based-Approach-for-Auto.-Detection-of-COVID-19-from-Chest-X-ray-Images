{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports-and-Basic-Configurations\" data-toc-modified-id=\"Imports-and-Basic-Configurations-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports and Basic Configurations</a></span><ul class=\"toc-item\"><li><span><a href=\"#For-Google-COLAB\" data-toc-modified-id=\"For-Google-COLAB-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>For Google COLAB</a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Imports</a></span></li></ul></li><li><span><a href=\"#Used-Classes-and-Functions\" data-toc-modified-id=\"Used-Classes-and-Functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Used Classes and Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Helper-Functions\" data-toc-modified-id=\"Helper-Functions-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Helper Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#LoadWorkState-+-StoreWorkState\" data-toc-modified-id=\"LoadWorkState-+-StoreWorkState-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>LoadWorkState + StoreWorkState</a></span></li><li><span><a href=\"#LogEvent-+-LogPaths-+-LogBestSolution\" data-toc-modified-id=\"LogEvent-+-LogPaths-+-LogBestSolution-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>LogEvent + LogPaths + LogBestSolution</a></span></li><li><span><a href=\"#GetCurrentState\" data-toc-modified-id=\"GetCurrentState-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>GetCurrentState</a></span></li><li><span><a href=\"#LoadCreateSplitFile\" data-toc-modified-id=\"LoadCreateSplitFile-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>LoadCreateSplitFile</a></span></li><li><span><a href=\"#HandleCurrentState\" data-toc-modified-id=\"HandleCurrentState-2.1.5\"><span class=\"toc-item-num\">2.1.5&nbsp;&nbsp;</span>HandleCurrentState</a></span></li><li><span><a href=\"#PlotFigure-+-PlotRandomFigure-+-PlotHistory\" data-toc-modified-id=\"PlotFigure-+-PlotRandomFigure-+-PlotHistory-2.1.6\"><span class=\"toc-item-num\">2.1.6&nbsp;&nbsp;</span>PlotFigure + PlotRandomFigure + PlotHistory</a></span></li><li><span><a href=\"#External-Metrics\" data-toc-modified-id=\"External-Metrics-2.1.7\"><span class=\"toc-item-num\">2.1.7&nbsp;&nbsp;</span>External Metrics</a></span></li><li><span><a href=\"#Levy\" data-toc-modified-id=\"Levy-2.1.8\"><span class=\"toc-item-num\">2.1.8&nbsp;&nbsp;</span>Levy</a></span></li><li><span><a href=\"#MapSolutionAsString-+-MapSolutionForCNN\" data-toc-modified-id=\"MapSolutionAsString-+-MapSolutionForCNN-2.1.9\"><span class=\"toc-item-num\">2.1.9&nbsp;&nbsp;</span>MapSolutionAsString + MapSolutionForCNN</a></span></li><li><span><a href=\"#BuildCompileTransferLearningCNN\" data-toc-modified-id=\"BuildCompileTransferLearningCNN-2.1.10\"><span class=\"toc-item-num\">2.1.10&nbsp;&nbsp;</span>BuildCompileTransferLearningCNN</a></span></li></ul></li><li><span><a href=\"#Fitness-Functions\" data-toc-modified-id=\"Fitness-Functions-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Fitness Functions</a></span></li><li><span><a href=\"#PopulationBasedOptimizer-Class\" data-toc-modified-id=\"PopulationBasedOptimizer-Class-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>PopulationBasedOptimizer Class</a></span></li><li><span><a href=\"#MantaRayForagingOptimizer-Class\" data-toc-modified-id=\"MantaRayForagingOptimizer-Class-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>MantaRayForagingOptimizer Class</a></span></li></ul></li><li><span><a href=\"#Learning-and-Optimization-Configurations\" data-toc-modified-id=\"Learning-and-Optimization-Configurations-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Learning and Optimization Configurations</a></span></li><li><span><a href=\"#PBO-Checking-and-Object-Creation\" data-toc-modified-id=\"PBO-Checking-and-Object-Creation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>PBO Checking and Object Creation</a></span></li><li><span><a href=\"#Run\" data-toc-modified-id=\"Run-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Run</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Basic Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Google COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T01:44:35.221581Z",
     "start_time": "2021-02-14T01:44:35.205593Z"
    },
    "id": "d6zMjxTcc4Bu"
   },
   "outputs": [],
   "source": [
    "import os, random\n",
    "\n",
    "TF_VERSION = \"latest\"\n",
    "NUM_THREADS = 1\n",
    "DEBUG = True\n",
    "COLAB = False\n",
    "RANDOM_SEED = random.randint(1, 1e4) # 1337\n",
    "BASE_DIR = os.getcwd()\n",
    "COLAB_DIR = \"\"\n",
    "\n",
    "if COLAB:\n",
    "  BASE_DIR = COLAB_DIR\n",
    "  from google.colab import drive\n",
    "  drive.mount(\"/content/drive\")\n",
    "  if (TF_VERSION == \"latest\"):\n",
    "    %tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2, os, itertools, random, pickle, warnings, datetime, math, gc, copy\n",
    "import timeit, traceback, contextlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models, applications, callbacks, metrics\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LeakyReLU, PReLU\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import L1L2, l2, l1, l1_l2\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetB0, EfficientNetB1, \\\n",
    "  EfficientNetB2, EfficientNetB3, EfficientNetB4, EfficientNetB5, \\\n",
    "  EfficientNetB6, EfficientNetB7, MobileNet, MobileNetV2, \\\n",
    "  VGG16, VGG19, ResNet50, ResNet101, ResNet152, \\\n",
    "  ResNet50V2, ResNet101V2, ResNet152V2, Xception, DenseNet121, DenseNet169, \\\n",
    "  DenseNet201, NASNetMobile, NASNetLarge, InceptionV3, InceptionResNetV2, \\\n",
    "  MobileNetV3Small, MobileNetV3Large\n",
    "from tensorflow.keras.activations import elu, relu, selu, gelu, sigmoid, \\\n",
    "  hard_sigmoid, linear, softmax, softplus, softsign, swish, tanh, \\\n",
    "  exponential\n",
    "from tensorflow.keras.optimizers import Adadelta, Nadam, Adam, \\\n",
    "  Adamax, Adagrad, Ftrl, SGD, RMSprop\n",
    "from tensorflow.keras.initializers import GlorotNormal, GlorotUniform, Zeros, \\\n",
    "  HeNormal, HeUniform, LecunNormal, LecunUniform, Identity, \\\n",
    "  Ones, Orthogonal, TruncatedNormal, VarianceScaling, \\\n",
    "  RandomNormal, RandomUniform\n",
    "from tensorflow.keras.metrics import Accuracy, AUC, Precision, \\\n",
    "  Recall, CosineSimilarity, CategoricalCrossentropy, KLDivergence, \\\n",
    "  CategoricalHinge, Hinge, SquaredHinge, Poisson, TruePositives, \\\n",
    "  TrueNegatives, FalsePositives, FalseNegatives, BinaryAccuracy, \\\n",
    "  BinaryCrossentropy, CategoricalAccuracy, LogCoshError, Mean, \\\n",
    "  MeanAbsoluteError, MeanAbsolutePercentageError, MeanIoU, \\\n",
    "  MeanSquaredError, MeanSquaredLogarithmicError, \\\n",
    "  MeanTensor, RootMeanSquaredError, SparseCategoricalAccuracy, \\\n",
    "  SparseCategoricalCrossentropy, SparseTopKCategoricalAccuracy, Sum, \\\n",
    "  TopKCategoricalAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Used Classes and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoadWorkState + StoreWorkState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadWorkState(statePickle):\n",
    "  f = open(statePickle, \"rb\")\n",
    "  L = pickle.load(f)\n",
    "  f.close()\n",
    "  return L\n",
    "\n",
    "\n",
    "def StoreWorkState(L, statePickle):\n",
    "  f = open(statePickle, \"wb\")\n",
    "  L = pickle.dump(L, f)\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogEvent + LogPaths + LogBestSolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogEvent(loggerFile, data, storeTimestamp=True):\n",
    "  f = open(loggerFile, \"a\")\n",
    "  if (storeTimestamp == True):\n",
    "    now = str(datetime.datetime.now())\n",
    "    f.write(f\"Timestamp {now}:\\t\")\n",
    "  f.write(f\"{str(data)}\\n\")\n",
    "  f.close()\n",
    "\n",
    "\n",
    "def LogPaths(currentState, iteration, points, score, storeTimestamp=True):\n",
    "  if (not os.path.exists(currentState[\"PATHS_FILE\"])):\n",
    "    f = open(currentState[\"PATHS_FILE\"], \"a\")\n",
    "    f.write(\"Iteration,Points,Score,Timestamp\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.close()\n",
    "    \n",
    "  f = open(currentState[\"PATHS_FILE\"], \"a\")\n",
    "  f.write(f\"{iteration},\")\n",
    "  strPoints = \"_\".join([str(p) for p in list(points)])\n",
    "  f.write(f\"{strPoints},\")\n",
    "  f.write(f\"{score},\")\n",
    "  if (storeTimestamp == True):\n",
    "    now = str(datetime.datetime.now())\n",
    "    f.write(f\"{now}\")\n",
    "  f.write(f\"\\n\")\n",
    "  f.close()\n",
    "  \n",
    "  \n",
    "def LogBestSolution(currentState, iteration, L, storeTimestamp=True):\n",
    "  if (not os.path.exists(currentState[\"BEST_FILE\"])):\n",
    "    f = open(currentState[\"BEST_FILE\"], \"a\")\n",
    "    f.write(\"#,Params Optimizer,Batch Size,Dropout,Model Learn Ratio,\")\n",
    "    f.write(\"Hidden Activation,Params Initializer,Regularizer,Loss,\")\n",
    "    for i in range(6):\n",
    "      f.write(currentState['METRICS_KEYWORDS'][i] + \",\")\n",
    "    f.write(\"WS,\")\n",
    "    for i in range(6, len(currentState['METRICS_KEYWORDS'])):\n",
    "      f.write(currentState['METRICS_KEYWORDS'][i] + \",\")\n",
    "    f.write(\"Timestamp\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "  f = open(currentState[\"BEST_FILE\"], \"a\")\n",
    "  f.write(f\"{str(iteration)},\")\n",
    "  for el in L:\n",
    "    f.write(f\"{str(el)},\")\n",
    "  if (storeTimestamp == True):\n",
    "    now = str(datetime.datetime.now())\n",
    "    f.write(f\"{now}\")\n",
    "  f.write(f\"\\n\")\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GetCurrentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCurrentState():\n",
    "  L = {\n",
    "    \"TF_VERSION\": TF_VERSION,\n",
    "    \"RANDOM_SEED\": RANDOM_SEED,\n",
    "    \"DEBUG\": DEBUG,\n",
    "    \"COLAB\": COLAB,\n",
    "    \"BASE_DIR\": BASE_DIR,\n",
    "    \"COLAB_DIR\": COLAB_DIR,\n",
    "    \"OWNER\": OWNER,\n",
    "    \"PBO\": PBO,\n",
    "    \"MODEL\": MODEL,\n",
    "    \"DATASET_PICKLE_FILE\": DATASET_PICKLE_FILE,\n",
    "    \"PICKELS_BASE_DIR\": PICKELS_BASE_DIR,\n",
    "    \"RESULTS_BASE_DIR\": RESULTS_BASE_DIR,\n",
    "    \"IMAGE_SHAPE\": IMAGE_SHAPE,\n",
    "    \"CATEGORIES\": CATEGORIES,\n",
    "    \"ITERS\": ITERS,\n",
    "    \"EPOCHS\": EPOCHS,\n",
    "    \"POPULATION_SIZE\": POPULATION_SIZE,\n",
    "    \"DIM\": DIM,\n",
    "    \"SKIP\": SKIP,\n",
    "    \"POPULATION\": POPULATION,\n",
    "    \"IS_NEW\": IS_NEW,\n",
    "    \"TRAIN_RATIO\": TRAIN_RATIO,\n",
    "    \"DROPOUT_RANGE\": DROPOUT_RANGE,\n",
    "    \"OUTPUT_ACTIVATION\": OUTPUT_ACTIVATION,\n",
    "    \"PRETRAINED_WEIGHTS\": PRETRAINED_WEIGHTS,\n",
    "    \"BATCH_SIZES\": BATCH_SIZES,\n",
    "    \"LEARN_RATIOS\": LEARN_RATIOS,\n",
    "    \"OPTIMIZERS\": OPTIMIZERS,\n",
    "    \"HIDDEN_ACTIVATIONS\": HIDDEN_ACTIVATIONS,\n",
    "    \"WEIGHT_INITIALIZERS\": WEIGHT_INITIALIZERS,\n",
    "    \"RESULTS_DIR\": RESULTS_DIR,\n",
    "    \"DATASET_PICKLE\": DATASET_PICKLE,\n",
    "    \"OWNER_DIR\": OWNER_DIR,\n",
    "    \"HDF5_RESULTS_DIR\": HDF5_RESULTS_DIR,\n",
    "    \"FIGURES_RESULTS_DIR\": FIGURES_RESULTS_DIR,\n",
    "    \"HISTORY_RESULTS_DIR\": HISTORY_RESULTS_DIR,\n",
    "    \"STATE_PICKLE\": STATE_PICKLE,\n",
    "    \"LOGGER_FILE\": LOGGER_FILE,\n",
    "    \"BEST_FILE\": BEST_FILE,\n",
    "    \"PATHS_FILE\": PATHS_FILE,\n",
    "    \"SPLIT_DATASET_PICKLE\": SPLIT_DATASET_PICKLE,\n",
    "    \"AUGMENTATION_CONFIG\": AUGMENTATION_CONFIG,\n",
    "    \"GA\": {},\n",
    "    \"MRFO\": {},\n",
    "    \"HHO\": {\n",
    "      \"RABBIT_LOCATION\": None,\n",
    "      \"CONVERGENCE_CURVE\": None,\n",
    "      \"RABBIT_ENERGY\": float(\"-inf\")\n",
    "    },\n",
    "    \"INTERPOLATION\": INTERPOLATION,\n",
    "    \"USE_TF\": USE_TF,\n",
    "    \"REGULARIZERS\": REGULARIZERS,\n",
    "    \"LOSS\": LOSS,\n",
    "    \"METRICS_KEYWORDS\": METRICS_KEYWORDS,\n",
    "    \"METRICS_NAMES\": METRICS_NAMES,\n",
    "    \"IS_DL\": IS_DL,\n",
    "    \"IS_MAX_PROBLEM\": IS_MAX_PROBLEM,\n",
    "    \"LOWER_BOUND\": LOWER_BOUND,\n",
    "    \"UPPER_BOUND\": UPPER_BOUND,\n",
    "  }\n",
    "  return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoadCreateSplitFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadCreateSplitFile(datasetPickle,\n",
    "            splitPickle,\n",
    "            noOfClasses,\n",
    "            trainRatio,\n",
    "            debug=True):\n",
    "  if (os.path.exists(splitPickle)):\n",
    "    if (debug): print(f\"Loading {splitPickle}\")\n",
    "    f = open(splitPickle, \"rb\")\n",
    "    [Xtrain, Xvalidation, Xtest, Ytrain, Yvalidation,\n",
    "     Ytest] = pickle.load(f)\n",
    "    f.close()\n",
    "    Q = [Xtrain, Xvalidation, Xtest, Ytrain, Yvalidation, Ytest]\n",
    "  else:\n",
    "    if (debug): print(f\"Creating {splitPickle}\")\n",
    "    randomState = 42\n",
    "    f = open(datasetPickle, \"rb\")\n",
    "    dataset = pickle.load(f)\n",
    "    f.close()\n",
    "    random.shuffle(dataset)\n",
    "    images, classes = zip(*dataset)\n",
    "    images = np.array(images, dtype='float32')\n",
    "    images = (images / 255.0).astype('float32')\n",
    "    classes = to_categorical(classes, noOfClasses)\n",
    "    classes = np.array(classes)\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(\n",
    "      images, classes, train_size=trainRatio, random_state=randomState)\n",
    "    Xtrain, Xvalidation, Ytrain, Yvalidation = train_test_split(\n",
    "      Xtrain, Ytrain, train_size=trainRatio, random_state=randomState)\n",
    "    Q = [Xtrain, Xvalidation, Xtest, Ytrain, Yvalidation, Ytest]\n",
    "    f = open(splitPickle, \"wb\")\n",
    "    pickle.dump(Q, f)\n",
    "    f.close()\n",
    "  return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HandleCurrentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HandleCurrentState(isNew, statePickle, loggerFile, debug=True):\n",
    "  if (isNew):\n",
    "    currentState = GetCurrentState()\n",
    "    StoreWorkState(currentState, statePickle)\n",
    "    state = \"The current state is created as required!\"\n",
    "    LogEvent(loggerFile, state)\n",
    "    LogEvent(loggerFile, currentState)\n",
    "  else:\n",
    "    if (os.path.exists(statePickle)):\n",
    "      currentState = LoadWorkState(statePickle)\n",
    "      state = \"The current state is loaded as the state file is found!\"\n",
    "      LogEvent(loggerFile, state)\n",
    "      LogEvent(loggerFile, currentState)\n",
    "    else:\n",
    "      currentState = GetCurrentState()\n",
    "      StoreWorkState(currentState, statePickle)\n",
    "      state = \"The current state is created again as the state file is not found!\"\n",
    "      LogEvent(loggerFile, state)\n",
    "      LogEvent(loggerFile, currentState)\n",
    "  if (debug): print(state)\n",
    "  if (debug): print(str(currentState))\n",
    "  return currentState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PlotFigure + PlotRandomFigure + PlotHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotFigure(image, category, debug=True, dpi=100):\n",
    "  fig = plt.figure(dpi=dpi)\n",
    "  plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "  if (category): plt.title(f\"Image Label '{category}'\")\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.show()\n",
    "  plt.clf()\n",
    "  plt.close()\n",
    "\n",
    "\n",
    "def PlotRandomFigure(X, y, categories, debug=True):\n",
    "  rand = np.random.randint(0, X.shape[0])\n",
    "  image = X[rand]\n",
    "  category = categories[np.argmax(y[rand])]\n",
    "  if (debug):\n",
    "    print(\"Max:\", np.max(image))\n",
    "    print(\"Min:\", np.min(image))\n",
    "    print(\"Mean:\", np.mean(image))\n",
    "    print(\"Standard Deviation:\", np.std(image))\n",
    "  PlotFigure(image, category, debug)\n",
    "\n",
    "\n",
    "def PlotHistory(currentState, figPath, history, figSize=(25, 15)):\n",
    "  fig = plt.figure(num=None, figsize=figSize)\n",
    "  keywords = copy.copy(currentState[\"METRICS_KEYWORDS\"])\n",
    "  keywords.insert(0, \"Loss\")\n",
    "  figs = copy.copy(currentState[\"METRICS_NAMES\"])\n",
    "  figs.insert(0, \"loss\")\n",
    "  locs = [\"lower\"] * len(keywords)\n",
    "  locs[0] = \"upper\"\n",
    "  titles = (\"(a)\", \"(b)\", \"(c)\", \"(d)\", \"(e)\", \"(f)\", \"(g)\", \"(h)\", \"(i)\",\n",
    "        \"(j)\", \"(k)\", \"(l)\", \"(m)\", \"(n)\", \"(o)\", \"(p)\", \"(q)\", \"(r)\",\n",
    "        \"(s)\", \"(t)\", \"(v)\", \"(w)\", \"(x)\", \"(y)\", \"(z)\")\n",
    "  for i in range(7):  # The loss and first 6 metrics.\n",
    "    keyword = keywords[i]\n",
    "    plt.subplot(2, 4, i + 1)  # Don't forget to handle 2 x 4.\n",
    "    plt.plot(history.history[figs[i]], label=keyword)\n",
    "    plt.plot(history.history[f'val_{figs[i]}'],\n",
    "         label=f'Validation {keyword}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(keyword)\n",
    "    plt.grid(\"both\")\n",
    "    plt.title(f\"{titles[i]} {keywords[i]} Curves\")\n",
    "    plt.legend(loc=f'{locs[i]} right')\n",
    "  plt.tight_layout()\n",
    "  plt.savefig(figPath, dpi=500)\n",
    "  #plt.show()\n",
    "  plt.clf()\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_coef_m(yTrue, yPred, smooth=1):\n",
    "  intersection = K.sum(K.abs(yTrue * yPred), axis=-1)\n",
    "  union = K.sum(yTrue, axis=-1) + K.sum(yPred, axis=-1) - intersection\n",
    "  iou = K.mean((intersection + smooth) / (union + smooth), axis=0)\n",
    "  return iou\n",
    "\n",
    "\n",
    "def dice_coef_m(yTrue, yPred, smooth=1):\n",
    "  intersection = K.sum(yTrue * yPred, axis=-1)\n",
    "  union = K.sum(yTrue, axis=-1) + K.sum(yPred, axis=-1)\n",
    "  dice = K.mean((2.0 * intersection + smooth) / (union + smooth), axis=0)\n",
    "  return dice\n",
    "\n",
    "\n",
    "def sensitivity_m(yTrue, yPred):\n",
    "  truePositives = K.sum(K.round(K.clip(yTrue * yPred, 0, 1)))\n",
    "  possiblePositives = K.sum(K.round(K.clip(yTrue, 0, 1)))\n",
    "  sensitivity = truePositives / (possiblePositives + K.epsilon())\n",
    "  return sensitivity\n",
    "\n",
    "\n",
    "def specificity_m(yTrue, yPred):\n",
    "  trueNegatives = K.sum(K.round(K.clip((1.0 - yTrue) * (1.0 - yPred), 0, 1)))\n",
    "  possibleNegatives = K.sum(K.round(K.clip(1 - yTrue, 0, 1)))\n",
    "  specificity = trueNegatives / (possibleNegatives + K.epsilon())\n",
    "  return specificity\n",
    "\n",
    "\n",
    "def recall_m(yTrue, yPred):\n",
    "  truePositives = K.sum(K.round(K.clip(yTrue * yPred, 0, 1)))\n",
    "  possiblePositives = K.sum(K.round(K.clip(yTrue, 0, 1)))\n",
    "  recall = truePositives / (possiblePositives + K.epsilon())\n",
    "  return recall\n",
    "\n",
    "\n",
    "def precision_m(yTrue, yPred):\n",
    "  truePositives = K.sum(K.round(K.clip(yTrue * yPred, 0, 1)))\n",
    "  predictedPositives = K.sum(K.round(K.clip(yPred, 0, 1)))\n",
    "  precision = truePositives / (predictedPositives + K.epsilon())\n",
    "  return precision\n",
    "\n",
    "\n",
    "def f1_m(yTrue, yPred):\n",
    "  precision = precision_m(yTrue, yPred)\n",
    "  recall = recall_m(yTrue, yPred)\n",
    "  return 2.0 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Levy(dim):\n",
    "  beta = 1.5\n",
    "  gamma = math.gamma(1.0 + beta)\n",
    "  num = gamma * math.sin(math.pi * beta / 2.0)\n",
    "  den = (gamma / 2.0) * beta * (2.0**((beta - 1.0) / 2.0))\n",
    "  sigma = (num / den)**(1.0 / beta)\n",
    "  u = 0.01 * np.random.randn(dim) * sigma\n",
    "  v = np.random.randn(dim)\n",
    "  zz = np.power(np.absolute(v), (1.0 / beta))\n",
    "  step = np.divide(u, zz)\n",
    "  return step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapSolutionAsString + MapSolutionForCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MapSolutionAsString(currentState, solution):\n",
    "  if (not currentState[\"IS_DL\"]):\n",
    "    return list(copy.copy(solution))\n",
    "  \n",
    "  mappedSolution = None\n",
    "  if (isinstance(currentState[\"DIM\"], int)):\n",
    "    mappedSolution = [None, None, None, None, None, None, None]\n",
    "    for i in range(currentState[\"DIM\"]):\n",
    "      mappedSolution[i] = \"auto\"\n",
    "  elif (isinstance(currentState[\"DIM\"], list)\n",
    "      or isinstance(currentState[\"DIM\"], tuple)):\n",
    "    mappedSolution = copy.copy(currentState[\"DIM\"])\n",
    "    for i in range(len(mappedSolution)):\n",
    "      if (mappedSolution[i] == \"N/A\"): mappedSolution[i] = None\n",
    "  if (mappedSolution[0] == \"auto\"):\n",
    "    r1 = math.floor(solution[0] * (len(currentState[\"OPTIMIZERS\"]) - 1))\n",
    "    optimizer = currentState[\"OPTIMIZERS\"][r1]\n",
    "    mappedSolution[0] = optimizer\n",
    "  if (mappedSolution[1] == \"auto\"):\n",
    "    r2 = math.floor(solution[1] * (len(currentState[\"BATCH_SIZES\"]) - 1))\n",
    "    batchSize = currentState[\"BATCH_SIZES\"][r2]\n",
    "    mappedSolution[1] = batchSize\n",
    "  if (mappedSolution[2] == \"auto\"):\n",
    "    low = currentState[\"DROPOUT_RANGE\"][0]\n",
    "    high = currentState[\"DROPOUT_RANGE\"][1]\n",
    "    step = 0.25\n",
    "    dropouts = np.arange(low, high + step, step) / 10.0\n",
    "    rr = math.floor(solution[2] * (len(dropouts) - 1))\n",
    "    dropout = dropouts[rr]\n",
    "    mappedSolution[2] = dropout\n",
    "  if (mappedSolution[3] == \"auto\"):\n",
    "    r3 = math.floor(solution[3] * (len(currentState[\"LEARN_RATIOS\"]) - 1))\n",
    "    learnRatio = currentState[\"LEARN_RATIOS\"][r3]\n",
    "    mappedSolution[3] = learnRatio\n",
    "  if (mappedSolution[4] == \"auto\"):\n",
    "    r4 = math.floor(solution[4] *\n",
    "            (len(currentState[\"HIDDEN_ACTIVATIONS\"]) - 1))\n",
    "    activation = currentState[\"HIDDEN_ACTIVATIONS\"][r4]\n",
    "    mappedSolution[4] = activation\n",
    "  if (mappedSolution[5] == \"auto\"):\n",
    "    r5 = math.floor(solution[5] *\n",
    "            (len(currentState[\"WEIGHT_INITIALIZERS\"]) - 1))\n",
    "    weightInitializer = currentState[\"WEIGHT_INITIALIZERS\"][r5]\n",
    "    mappedSolution[5] = weightInitializer\n",
    "  if (mappedSolution[6] == \"auto\"):\n",
    "    r6 = math.floor(solution[6] * (len(currentState[\"REGULARIZERS\"]) - 1))\n",
    "    regularizer = currentState[\"REGULARIZERS\"][r6]\n",
    "    mappedSolution[6] = regularizer\n",
    "  return copy.copy(mappedSolution)\n",
    "\n",
    "\n",
    "def MapSolutionForCNN(currentState, solution):\n",
    "  mappedSolution = MapSolutionAsString(currentState, solution)\n",
    "\n",
    "  if (mappedSolution[4] == \"prelu\"):\n",
    "    mappedSolution[4] = PReLU()\n",
    "  elif (mappedSolution[4] == \"leakyrelu\"):\n",
    "    mappedSolution[4] = LeakyReLU()\n",
    "\n",
    "  if (mappedSolution[6] is not None):\n",
    "    splitted = mappedSolution[6].split(\"_\")\n",
    "    if (splitted[0] == \"l1\"): regularizer = l1(float(splitted[1]))\n",
    "    elif (splitted[0] == \"l2\"): regularizer = l2(float(splitted[1]))\n",
    "    else: regularizer = None\n",
    "    mappedSolution[6] = regularizer\n",
    "\n",
    "  return copy.copy(mappedSolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BuildCompileTransferLearningCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T19:43:47.767755Z",
     "start_time": "2021-02-09T19:43:46.848299Z"
    },
    "code_folding": [
     0,
     6,
     11,
     19,
     63,
     92,
     115,
     125,
     136,
     159,
     165,
     171,
     176,
     237
    ],
    "id": "86nJeb2Gc02E"
   },
   "outputs": [],
   "source": [
    "def BuildCompileTransferLearningCNN(model,\n",
    "                  inputShape,\n",
    "                  noOfClasses,\n",
    "                  hiddenActivation,\n",
    "                  outputActivation,\n",
    "                  weightInitializer,\n",
    "                  pretrainedWeights,\n",
    "                  optimizer,\n",
    "                  dropout,\n",
    "                  learnRatio,\n",
    "                  regularizer,\n",
    "                  loss,\n",
    "                  debug=True):\n",
    "\n",
    "  baseModel = None\n",
    "  includeTop = False\n",
    "\n",
    "  if (model == \"MobileNetV2\"):\n",
    "    baseModel = MobileNetV2(weights=pretrainedWeights,\n",
    "                include_top=includeTop)\n",
    "  elif (model == \"MobileNet\"):\n",
    "    baseModel = MobileNet(weights=pretrainedWeights,\n",
    "                include_top=includeTop)\n",
    "  elif (model == \"MobileNetV3Small\"):\n",
    "    baseModel = MobileNetV3Small(weights=pretrainedWeights,\n",
    "                   include_top=includeTop)\n",
    "  elif (model == \"MobileNetV3Large\"):\n",
    "    baseModel = MobileNetV3Large(weights=pretrainedWeights,\n",
    "                   include_top=includeTop)\n",
    "  elif (model == \"VGG16\"):\n",
    "    baseModel = VGG16(weights=pretrainedWeights, include_top=includeTop)\n",
    "  elif (model == \"VGG19\"):\n",
    "    baseModel = VGG19(weights=pretrainedWeights, include_top=includeTop)\n",
    "  elif (model == \"ResNet50\"):\n",
    "    baseModel = ResNet50(weights=pretrainedWeights, include_top=includeTop)\n",
    "  elif (model == \"ResNet101\"):\n",
    "    baseModel = ResNet101(weights=pretrainedWeights,\n",
    "                include_top=includeTop)\n",
    "  elif (model == \"ResNet152\"):\n",
    "    baseModel = ResNet152(weights=pretrainedWeights,\n",
    "                include_top=includeTop)\n",
    "  elif (model == \"ResNet50V2\"):\n",
    "    baseModel = ResNet50V2(weights=pretrainedWeights,\n",
    "                 include_top=includeTop)\n",
    "  elif (model == \"ResNet101V2\"):\n",
    "    baseModel = ResNet101V2(weights=pretrainedWeights,\n",
    "                include_top=includeTop)\n",
    "  elif (model == \"ResNet152V2\"):\n",
    "    baseModel = ResNet152V2(weights=pretrainedWeights,\n",
    "                include_top=includeTop)\n",
    "  elif (model == \"Xception\"):\n",
    "    baseModel = Xception(weights=pretrainedWeights, include_top=includeTop)\n",
    "  elif (model == \"DenseNet121\"):\n",
    "    baseModel = DenseNet121(weights=pretrainedWeights,\n",
    "                include_top=includeTop)\n",
    "  elif (model == \"DenseNet169\"):\n",
    "    baseModel = DenseNet169(weights=pretrainedWeights,\n",
    "                include_top=includeTop)\n",
    "  elif (model == \"DenseNet201\"):\n",
    "    baseModel = DenseNet201(weights=pretrainedWeights,\n",
    "                include_top=includeTop)\n",
    "  elif (model == \"NASNetLarge\"):\n",
    "    baseModel = NASNetLarge(weights=pretrainedWeights,\n",
    "                include_top=includeTop)\n",
    "  elif (model == \"InceptionV3\"):\n",
    "    baseModel = InceptionV3(weights=pretrainedWeights,\n",
    "                include_top=includeTop)\n",
    "  elif (model == \"InceptionResNetV2\"):\n",
    "    baseModel = InceptionResNetV2(weights=pretrainedWeights,\n",
    "                    include_top=includeTop)\n",
    "  elif (model == \"NASNetMobile\"):\n",
    "    baseModel = NASNetMobile(weights=pretrainedWeights,\n",
    "                 include_top=includeTop)\n",
    "  elif (model == \"EfficientNetB0\"):\n",
    "    baseModel = EfficientNetB0(weights=pretrainedWeights,\n",
    "                   include_top=includeTop)\n",
    "  elif (model == \"EfficientNetB1\"):\n",
    "    baseModel = EfficientNetB1(weights=pretrainedWeights,\n",
    "                   include_top=includeTop)\n",
    "  elif (model == \"EfficientNetB2\"):\n",
    "    baseModel = EfficientNetB2(weights=pretrainedWeights,\n",
    "                   include_top=includeTop)\n",
    "  elif (model == \"EfficientNetB3\"):\n",
    "    baseModel = EfficientNetB3(weights=pretrainedWeights,\n",
    "                   include_top=includeTop)\n",
    "  elif (model == \"EfficientNetB4\"):\n",
    "    baseModel = EfficientNetB4(weights=pretrainedWeights,\n",
    "                   include_top=includeTop)\n",
    "  elif (model == \"EfficientNetB5\"):\n",
    "    baseModel = EfficientNetB5(weights=pretrainedWeights,\n",
    "                   include_top=includeTop)\n",
    "  elif (model == \"EfficientNetB6\"):\n",
    "    baseModel = EfficientNetB6(weights=pretrainedWeights,\n",
    "                   include_top=includeTop)\n",
    "  elif (model == \"EfficientNetB7\"):\n",
    "    baseModel = EfficientNetB7(weights=pretrainedWeights,\n",
    "                   include_top=includeTop)\n",
    "\n",
    "  assert (baseModel is not None)\n",
    "\n",
    "  if (learnRatio is None): learnRatio = 100.0\n",
    "\n",
    "  for layer in baseModel.layers:\n",
    "    layer.trainable = False\n",
    "  L = len(baseModel.layers)\n",
    "  fromL = int(learnRatio * L / 100.0)\n",
    "  for layer in baseModel.layers[L - fromL:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "  if (dropout is None or dropout <= 0):\n",
    "    model = tf.keras.Sequential([\n",
    "      baseModel,\n",
    "      layers.GlobalAveragePooling2D(),\n",
    "      layers.Dense(noOfClasses, activation=outputActivation)\n",
    "    ])\n",
    "  else:\n",
    "    model = tf.keras.Sequential([\n",
    "      baseModel,\n",
    "      layers.GlobalAveragePooling2D(),\n",
    "      layers.Dropout(dropout),\n",
    "      layers.Dense(noOfClasses, activation=outputActivation)\n",
    "    ])\n",
    "\n",
    "  if (optimizer is not None):\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=METRICS)\n",
    "  else:\n",
    "    model.compile(loss=loss, metrics=METRICS)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T19:43:47.813469Z",
     "start_time": "2021-02-09T19:43:47.769757Z"
    },
    "code_folding": [
     0,
     3
    ],
    "id": "lXiEQF_Qc02S"
   },
   "outputs": [],
   "source": [
    "def FitnessFunction(iteration, solution, currentState):\n",
    "  if (currentState[\"DEBUG\"]): print(\"=\" * 50)\n",
    "  ResetEnvironment()\n",
    "\n",
    "  mappedSolution = MapSolutionForCNN(currentState, solution)\n",
    "  optimizer, batchSize, dropout, learnRatio, hiddenActivation, weightInitializer, regularizer = mappedSolution\n",
    "  mappedSolutionStr = MapSolutionAsString(currentState, solution)\n",
    "  optimizerStr, batchSizeStr, dropoutStr, learnRatioStr, hiddenActivationStr, weightInitializerStr, regularizerStr = mappedSolutionStr\n",
    "\n",
    "  aug = ImageDataGenerator(\n",
    "    rotation_range=currentState[\"AUGMENTATION_CONFIG\"]['rotation_range'],\n",
    "    width_shift_range=currentState[\"AUGMENTATION_CONFIG\"]\n",
    "    ['width_shift_range'],\n",
    "    height_shift_range=currentState[\"AUGMENTATION_CONFIG\"]\n",
    "    ['height_shift_range'],\n",
    "    shear_range=currentState[\"AUGMENTATION_CONFIG\"]['shear_range'],\n",
    "    zoom_range=currentState[\"AUGMENTATION_CONFIG\"]['zoom_range'],\n",
    "    horizontal_flip=currentState[\"AUGMENTATION_CONFIG\"]['horizontal_flip'],\n",
    "    vertical_flip=currentState[\"AUGMENTATION_CONFIG\"]['vertical_flip'],\n",
    "    brightness_range=currentState[\"AUGMENTATION_CONFIG\"]\n",
    "    ['brightness_range'],\n",
    "    fill_mode=currentState[\"AUGMENTATION_CONFIG\"]['fill_mode'],\n",
    "  )\n",
    "\n",
    "  trainIter = aug.flow(Xtrain, Ytrain, batch_size=batchSize)\n",
    "  owner = currentState[\"OWNER\"]\n",
    "  modelName = currentState[\"MODEL\"]\n",
    "  inputShape = currentState[\"IMAGE_SHAPE\"]\n",
    "  noOfClasses = len(currentState[\"CATEGORIES\"])\n",
    "  loss = currentState[\"LOSS\"]\n",
    "\n",
    "  if (optimizerStr is None): optimizerStr = \"NA\"\n",
    "  if (batchSizeStr is None): batchSizeStr = \"NA\"\n",
    "  if (dropoutStr is None): dropoutStr = \"NA\"\n",
    "  if (learnRatioStr is None): learnRatioStr = \"NA\"\n",
    "  if (hiddenActivationStr is None): hiddenActivationStr = \"NA\"\n",
    "  if (weightInitializerStr is None): weightInitializerStr = \"NA\"\n",
    "  if (regularizerStr is None): regularizerStr = \"NA\"\n",
    "\n",
    "  temp1 = f\"{str(owner)}_{str(modelName)}_{str(inputShape)}_{str(noOfClasses)}\"\n",
    "  temp1 += f\"_{optimizerStr}_{str(batchSizeStr)}_{str(dropoutStr)}_{str(learnRatioStr)}\"\n",
    "  temp1 += f\"_{str(hiddenActivationStr)}_{str(weightInitializerStr)}_{str(regularizerStr)}\"\n",
    "  temp2 = temp1 + f\"_{str(iteration)}\"\n",
    "\n",
    "  name1 = f\"{temp1}.hdf5\"\n",
    "  name2 = f\"{temp2}.hdf5\"\n",
    "  figName = f\"{temp2}.jpg\"\n",
    "  csvName = f\"{temp1}.csv\"\n",
    "  modelPlot = f\"model_plot.png\"\n",
    "\n",
    "  checkpointFilepath1 = os.path.join(currentState[\"HDF5_RESULTS_DIR\"], name1)\n",
    "  checkpointFilepath2 = os.path.join(currentState[\"HDF5_RESULTS_DIR\"], name2)\n",
    "  figPath = os.path.join(currentState[\"FIGURES_RESULTS_DIR\"], figName)\n",
    "  csvPath = os.path.join(currentState[\"HISTORY_RESULTS_DIR\"], csvName)\n",
    "  modelPlotPath = os.path.join(currentState[\"OWNER_DIR\"], modelPlot)\n",
    "\n",
    "  modelCheckpointCallback1 = callbacks.ModelCheckpoint(\n",
    "    filepath=checkpointFilepath1,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    verbose=0)\n",
    "  csvLoggerCallback = callbacks.CSVLogger(csvPath, append=True)\n",
    "  earlyStoppingCallback = callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "  callbacksList = [\n",
    "    modelCheckpointCallback1, csvLoggerCallback, earlyStoppingCallback\n",
    "  ]\n",
    "\n",
    "  toLog = f\"New {temp1}.\"\n",
    "  if (os.path.exists(checkpointFilepath1)):\n",
    "    toLog = f\"Updating using {temp1}.\"\n",
    "    if (currentState[\"DEBUG\"]): print(toLog)\n",
    "    model = load_model(checkpointFilepath1,\n",
    "               compile=False)  \n",
    "    if (optimizer is not None):\n",
    "      model.compile(optimizer=optimizer, loss=LOSS, metrics=METRICS)\n",
    "    else:\n",
    "      model.compile(loss=LOSS, metrics=metricsList)\n",
    "    model.load_weights(checkpointFilepath1)\n",
    "    ev = model.evaluate(Xvalidation, Yvalidation, verbose=0)\n",
    "    modelCheckpointCallback1.best = ev[1]  # Accuracy.\n",
    "    message = f\"Start the Model Checkpoint with a Best Value of {ev[1]}.\"\n",
    "    if (currentState[\"DEBUG\"]):\n",
    "      print(message)\n",
    "    LogEvent(currentState[\"LOGGER_FILE\"], message)\n",
    "  else:\n",
    "    ev = modelCheckpointCallback1.best\n",
    "    message = f\"Start the Model Checkpoint with a Best Value of {ev}.\"\n",
    "    if (currentState[\"DEBUG\"]):\n",
    "      print(toLog)\n",
    "      print(message)\n",
    "    LogEvent(currentState[\"LOGGER_FILE\"], message)\n",
    "    if (currentState[\"USE_TF\"]):\n",
    "      model = BuildCompileTransferLearningCNN(\n",
    "        modelName,\n",
    "        inputShape,\n",
    "        noOfClasses,\n",
    "        hiddenActivation,\n",
    "        currentState[\"OUTPUT_ACTIVATION\"],\n",
    "        weightInitializer,\n",
    "        currentState[\"PRETRAINED_WEIGHTS\"],\n",
    "        optimizer,\n",
    "        dropout,\n",
    "        learnRatio,\n",
    "        regularizer,\n",
    "        loss,\n",
    "        debug=currentState[\"DEBUG\"])\n",
    "    else:\n",
    "      model = None\n",
    "\n",
    "  if (not os.path.exists(modelPlotPath)):\n",
    "    plot_model(model,\n",
    "           to_file=modelPlotPath,\n",
    "           show_shapes=True,\n",
    "           show_layer_names=False)\n",
    "  history = model.fit(trainIter,\n",
    "            epochs=currentState[\"EPOCHS\"],\n",
    "            shuffle=True,\n",
    "            validation_data=(Xvalidation, Yvalidation),\n",
    "            callbacks=callbacksList,\n",
    "            verbose=0)\n",
    "\n",
    "  PlotHistory(currentState, figPath, history)\n",
    "\n",
    "  def __innerCalc(results):\n",
    "    return 0.1 * (1.0 / results[0]) + 0.4 * (results[1] * 100.0) + \\\n",
    "      0.1 * (results[2] * 100.0) + 0.1 * (results[3] * 100.0) + \\\n",
    "      0.1 * (results[4] * 100.0) + 0.1 * (results[5] * 100.0) + \\\n",
    "      0.1 * (results[6] * 100.0)\n",
    "\n",
    "  results = model.evaluate(Xtrain, Ytrain, verbose=0)\n",
    "  first = copy.copy(results[:])\n",
    "  fitnessValueTrain = __innerCalc(results)\n",
    "  results = model.evaluate(Xvalidation, Yvalidation, verbose=0)\n",
    "  second = copy.copy(results[:])\n",
    "  fitnessValueValidation = __innerCalc(results)\n",
    "  results = model.evaluate(Xtest, Ytest, verbose=0)\n",
    "  third = copy.copy(results[:])\n",
    "  fitnessValueTest = __innerCalc(results)\n",
    "  totalFitnessValue = (fitnessValueTrain + fitnessValueValidation +\n",
    "             fitnessValueTest) / 3.0\n",
    "  fitnessValue = np.round(totalFitnessValue, 5)\n",
    "\n",
    "  record = [0] * len(first)\n",
    "  for q in range(len(record)):\n",
    "    record[q] = float((first[q] + second[q] + third[q]) / 3.0)\n",
    "\n",
    "  for q in range(len(record)):\n",
    "    if (q in [1, 3, 4, 5]):\n",
    "      record[q] = round(record[q] * 100.0, 2)\n",
    "    else:\n",
    "      if (record[q] >= 100): record[q] = round(record[q], 1)\n",
    "      elif (record[q] >= 10): record[q] = round(record[q], 2)\n",
    "      elif (record[q] >= 1): record[q] = round(record[q], 3)\n",
    "      else: record[q] = round(record[q], 4)\n",
    "\n",
    "  record.insert(7, fitnessValue)  #WS\n",
    "  if (fitnessValue >= 100): record[7] = 100\n",
    "  elif (fitnessValue >= 10): record[7] = round(fitnessValue, 2)\n",
    "  elif (fitnessValue >= 1): record[7] = round(fitnessValue, 3)\n",
    "  else: record[7] = round(fitnessValue, 4)\n",
    "\n",
    "  LogEvent(currentState[\"LOGGER_FILE\"], \"=\" * 50)\n",
    "  LogEvent(currentState[\"LOGGER_FILE\"], toLog)\n",
    "  LogEvent(currentState[\"LOGGER_FILE\"], \"Template 1: \" + temp1)\n",
    "  LogEvent(currentState[\"LOGGER_FILE\"], \"Template 2: \" + temp2)\n",
    "  LogEvent(currentState[\"LOGGER_FILE\"], \"HDF5 1: \" + name1)\n",
    "  LogEvent(currentState[\"LOGGER_FILE\"], \"HDF5 2: \" + name2)\n",
    "  LogEvent(currentState[\"LOGGER_FILE\"], \"Figure Name: \" + figName)\n",
    "  LogEvent(currentState[\"LOGGER_FILE\"], \"Logger Name: \" + csvName)\n",
    "  LogEvent(currentState[\"LOGGER_FILE\"], history.history)\n",
    "  LogEvent(currentState[\"LOGGER_FILE\"], results)\n",
    "  LogEvent(currentState[\"LOGGER_FILE\"], fitnessValueTrain)\n",
    "  LogEvent(currentState[\"LOGGER_FILE\"], fitnessValueValidation)\n",
    "  LogEvent(currentState[\"LOGGER_FILE\"], fitnessValueTest)\n",
    "  LogEvent(currentState[\"LOGGER_FILE\"], fitnessValue)\n",
    "  LogEvent(currentState[\"LOGGER_FILE\"], \"=\" * 50)\n",
    "\n",
    "  if (currentState[\"DEBUG\"]):\n",
    "    print(fitnessValueTrain, fitnessValueValidation, fitnessValueTest,\n",
    "        fitnessValue)\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "  del model, history, aug\n",
    "  gc.collect()\n",
    "\n",
    "  return fitnessValue, copy.copy(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PopulationBasedOptimizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T19:43:47.874175Z",
     "start_time": "2021-02-09T19:43:47.846187Z"
    },
    "code_folding": [
     0
    ],
    "id": "qogO9pZ7x6GK"
   },
   "outputs": [],
   "source": [
    "class PopulationBasedOptimizer(object):\n",
    "  def __init__(self,\n",
    "         fitnessFunction,\n",
    "         currentState,\n",
    "         lowerBound=0.0,\n",
    "         upperBound=1.0):\n",
    "    self.fitnessFunction = fitnessFunction\n",
    "    self.currentState = currentState\n",
    "    self.lowerBound = self.currentState[\"LOWER_BOUND\"]\n",
    "    self.upperBound = self.currentState[\"UPPER_BOUND\"]\n",
    "\n",
    "    self.population = []\n",
    "    self.populationWithScores = []\n",
    "    self.currentIteration = 0\n",
    "\n",
    "  def GetPopulation(self):\n",
    "    return self.population.copy()\n",
    "\n",
    "  def GetPopulationWithScores(self):\n",
    "    return self.populationWithScores\n",
    "\n",
    "  def InitPopulation(self):\n",
    "    if (isinstance(self.currentState[\"DIM\"], int)):\n",
    "      R = self.currentState[\"DIM\"]\n",
    "    else:\n",
    "      R = len(self.currentState[\"DIM\"])\n",
    "    if (not isinstance(self.lowerBound, list)):\n",
    "      lb = [self.lowerBound for _ in range(R)]\n",
    "    if (not isinstance(self.upperBound, list)):\n",
    "      ub = [self.upperBound for _ in range(R)]\n",
    "    lb = np.asarray(lb)\n",
    "    ub = np.asarray(ub)\n",
    "\n",
    "    size = (self.currentState[\"POPULATION_SIZE\"], R)\n",
    "    uniformRandom = np.random.uniform(0, 1, size)\n",
    "    clippedUniformRandom = [x * (ub - lb) + lb for x in uniformRandom]\n",
    "    self.population = np.round(np.asarray(clippedUniformRandom), 3)\n",
    "\n",
    "  def CalculateFitnessScores(self):\n",
    "    populationWithFitnessValues = []\n",
    "    population = self.GetPopulation()\n",
    "    for j, solution in enumerate(population):\n",
    "      if (self.currentState[\"DEBUG\"]):\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"WORKING ON SOLUTION NO. {j+1}.\")\n",
    "        print(\"=\" * 50)\n",
    "        if (self.currentState[\"IS_DL\"]):\n",
    "          print(solution, MapSolutionAsString(self.currentState,\n",
    "                          solution))\n",
    "      fitnessScore, record = self.fitnessFunction(\n",
    "        self.currentIteration, solution, self.currentState)\n",
    "      rec = [solution, fitnessScore, record]\n",
    "      populationWithFitnessValues.append(rec)\n",
    "      if (self.currentState[\"DEBUG\"]):\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"SOLUTION NO. {j+1} HAS SCORE OF {fitnessScore}.\")\n",
    "        print(\"=\" * 50)\n",
    "    if (self.currentState[\"IS_MAX_PROBLEM\"]): # Maximization.\n",
    "      populationWithFitnessValues = sorted(populationWithFitnessValues,\n",
    "                       key=lambda s: s[1],\n",
    "                       reverse=True)\n",
    "    else: # Minimization\n",
    "      populationWithFitnessValues = sorted(populationWithFitnessValues,\n",
    "                       key=lambda s: s[1],\n",
    "                       reverse=False)\n",
    "    self.populationWithScores = populationWithFitnessValues\n",
    "    bestSolution = populationWithFitnessValues[0]\n",
    "    if (self.currentState[\"DEBUG\"]):\n",
    "      print(\"=\" * 50)\n",
    "      print(f\"BEST SOLUTION VALUES = {bestSolution[0]}.\")\n",
    "      print(f\"BEST SOLUTION SCORE = {bestSolution[1]}.\")\n",
    "      print(\"=\" * 50)\n",
    "    LogPaths(self.currentState, self.currentIteration, \n",
    "             bestSolution[0], bestSolution[1])\n",
    "\n",
    "  def Run(self):\n",
    "    if (self.currentState[\"SKIP\"] == 0\n",
    "        and self.currentState[\"POPULATION\"] is None):\n",
    "      if (self.currentState[\"DEBUG\"]): print(\"Population is created!\")\n",
    "      self.InitPopulation()\n",
    "    else:\n",
    "      if (self.currentState[\"DEBUG\"]): print(\"Population is loaded!\")\n",
    "      self.population = self.currentState[\"POPULATION\"]\n",
    "\n",
    "    for i in range(self.currentState[\"SKIP\"], self.currentState[\"ITERS\"]):\n",
    "      if (self.currentState[\"DEBUG\"]):\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"WORKING ON ITERATION NO. {i+1}.\")\n",
    "        print(\"=\" * 50)\n",
    "      self.currentIteration = i + 1\n",
    "      self.CalculateFitnessScores()\n",
    "      self.LogHistory()\n",
    "      LogEvent(self.currentState[\"LOGGER_FILE\"],\n",
    "           f\"Old Population @ {i+1}\")\n",
    "      LogEvent(self.currentState[\"LOGGER_FILE\"], self.GetPopulation())\n",
    "      self.UpdatePopulation()\n",
    "      LogEvent(self.currentState[\"LOGGER_FILE\"],\n",
    "           f\"New Population @ {i+1}\")\n",
    "      LogEvent(self.currentState[\"LOGGER_FILE\"], self.GetPopulation())\n",
    "      self.UpdateConfigurations()\n",
    "\n",
    "    LogEvent(self.currentState[\"LOGGER_FILE\"], \"=\" * 50)\n",
    "    LogEvent(self.currentState[\"LOGGER_FILE\"], \"Completed\")\n",
    "    LogEvent(self.currentState[\"LOGGER_FILE\"], \"=\" * 50)\n",
    "\n",
    "    if (currentState[\"DEBUG\"]): print(\"Completed!\")\n",
    "\n",
    "  def PrintPopulation(self):\n",
    "    if (self.currentState[\"DEBUG\"] and self.currentState[\"IS_DL\"]):\n",
    "      for j, temp in enumerate(self.GetPopulation()):\n",
    "        mappedSolution = MapSolutionForCNN(self.currentState, temp)\n",
    "        print(self.currentIteration, j + 1, temp, mappedSolution)\n",
    "\n",
    "  def LogHistory(self):\n",
    "    LogEvent(self.currentState[\"LOGGER_FILE\"], \"=\" * 50)\n",
    "    LogEvent(self.currentState[\"LOGGER_FILE\"], self.currentIteration)\n",
    "    LogEvent(self.currentState[\"LOGGER_FILE\"], self.GetPopulation())\n",
    "    LogEvent(self.currentState[\"LOGGER_FILE\"],\n",
    "         self.GetPopulationWithScores())\n",
    "    if (self.currentState[\"IS_DL\"]):\n",
    "      for j, solution in enumerate(self.GetPopulationWithScores()):\n",
    "        mappedSolution = MapSolutionForCNN(self.currentState, solution[0])\n",
    "        LogEvent(self.currentState[\"LOGGER_FILE\"],\n",
    "             [j + 1, mappedSolution, solution[1]])\n",
    "      LogEvent(self.currentState[\"LOGGER_FILE\"], \"=\" * 50)\n",
    "\n",
    "    bestSolution = self.GetPopulationWithScores()[0]\n",
    "    mappedBestSolution = MapSolutionAsString(self.currentState,\n",
    "                         bestSolution[0])\n",
    "    mappedBestSolution.extend(bestSolution[2])\n",
    "    if (self.currentState[\"IS_DL\"]):\n",
    "      LogBestSolution(currentState, self.currentIteration,\n",
    "            mappedBestSolution)\n",
    "\n",
    "  def UpdateConfigurations(self):\n",
    "    self.currentState[\"SKIP\"] = self.currentIteration\n",
    "    self.currentState[\"POPULATION\"] = self.GetPopulation().copy()\n",
    "    self.StoreWorkStateNow()\n",
    "\n",
    "  def StoreWorkStateNow(self):\n",
    "    StoreWorkState(self.currentState, self.currentState[\"STATE_PICKLE\"])\n",
    "\n",
    "  @abstractmethod\n",
    "  def UpdatePopulation(self):\n",
    "    raise NotImplementedError(\n",
    "      \"UpdatePopulation function must be implemented.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T19:52:36.653898Z",
     "start_time": "2021-02-13T19:52:36.645923Z"
    }
   },
   "source": [
    "## MantaRayForagingOptimizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MantaRayForagingOptimizer(PopulationBasedOptimizer):\n",
    "  def UpdatePopulation(self):\n",
    "    if (isinstance(self.currentState[\"DIM\"], int)):\n",
    "      dim = self.currentState[\"DIM\"]\n",
    "    else:\n",
    "      dim = len(self.currentState[\"DIM\"])\n",
    "    iters = self.currentState[\"ITERS\"]\n",
    "    populationSize = self.currentState[\"POPULATION_SIZE\"]\n",
    "    lb = self.lowerBound\n",
    "    ub = self.upperBound\n",
    "\n",
    "    scores = self.GetPopulationWithScores()\n",
    "    bestSolution, bestFitness = scores[0][0], scores[0][1]\n",
    "    for j, temp in enumerate(scores):\n",
    "      fitness = scores[j][1]\n",
    "      if (self.currentState[\"IS_MAX_PROBLEM\"]): # Maximization.\n",
    "        if (fitness > bestFitness):\n",
    "          bestSolution, bestFitness = scores[j][0], scores[j][1]\n",
    "      else:\n",
    "        if (fitness < bestFitness):\n",
    "          bestSolution, bestFitness = scores[j][0], scores[j][1]\n",
    "\n",
    "    if not isinstance(lb, list):\n",
    "      lb = [lb for _ in range(dim)]\n",
    "      ub = [ub for _ in range(dim)]\n",
    "    lb = np.asarray(lb)\n",
    "    ub = np.asarray(ub)\n",
    "\n",
    "    X = np.asarray([e[0].copy() for e in scores])\n",
    "\n",
    "    newX = X.copy()\n",
    "\n",
    "    coef = self.currentIteration / iters\n",
    "\n",
    "    for i in range(0, populationSize):\n",
    "      r = np.random.random() \n",
    "      alpha = 2.0 * r * np.sqrt(np.abs(np.log(r)))\n",
    "\n",
    "      r1 = np.random.random() \n",
    "      factor = (iters - self.currentIteration + 1.0) / (iters * 1.0)\n",
    "      beta = 2.0 * np.exp(r1 * factor) * np.sin(2.0 * np.pi * r1)\n",
    "\n",
    "      if (np.random.random() < 0.5):\n",
    "        if (coef < np.random.random()):\n",
    "          xRand = lb + np.random.random(dim) * (ub - lb)\n",
    "          if (i == 0):\n",
    "            newX[i, :] = xRand + r * (xRand - X[i, :]) + beta * (\n",
    "              xRand - X[i, :])\n",
    "          else:\n",
    "            newX[i, :] = xRand + r * (\n",
    "              X[i - 1, :] - X[i, :]) + beta * (xRand - X[i, :])\n",
    "        else:\n",
    "          if (i == 0):\n",
    "            newX[i, :] = bestSolution + r * (bestSolution - X[\n",
    "              i, :]) + beta * (bestSolution - X[i, :])\n",
    "          else:\n",
    "            newX[i, :] = bestSolution + r * (X[i - 1, :] - X[\n",
    "              i, :]) + beta * (bestSolution - X[i, :])\n",
    "      else:\n",
    "        if (i == 0):\n",
    "          newX[i, :] = X[i, :] + r * (bestSolution - X[\n",
    "            i, :]) + alpha * (bestSolution - X[i, :])\n",
    "        else:\n",
    "          newX[i, :] = X[i, :] + r * (X[i - 1, :] - X[\n",
    "            i, :]) + alpha * (bestSolution - X[i, :])\n",
    "\n",
    "      newX[i, :] = np.clip(newX[i, :], lb, ub)\n",
    "      currentScore = self.fitnessFunction(self.currentIteration,\n",
    "                        newX[i, :],\n",
    "                        self.currentState)[0]\n",
    "\n",
    "      if (self.currentState[\"IS_MAX_PROBLEM\"]): # Maximization.\n",
    "        if (currentScore > bestFitness):\n",
    "          bestSolution, bestFitness = newX[i, :], currentScore\n",
    "      else:\n",
    "        if (currentScore < bestFitness):\n",
    "          bestSolution, bestFitness = newX[i, :], currentScore\n",
    "\n",
    "      s = 2.0\n",
    "      r2 = np.random.random()\n",
    "      r3 = np.random.random() \n",
    "      newX[i, :] = X[i, :] + s * (r2 * bestSolution - r3 * X[i, :])\n",
    "\n",
    "      newX[i, :] = np.clip(newX[i, :], lb, ub)\n",
    "      currentScore = self.fitnessFunction(self.currentIteration,\n",
    "                        newX[i, :],\n",
    "                        self.currentState)[0]\n",
    "\n",
    "      if (self.currentState[\"IS_MAX_PROBLEM\"]): # Maximization.\n",
    "        if (currentScore > bestFitness):\n",
    "          bestSolution, bestFitness = newX[i, :], currentScore\n",
    "      else:\n",
    "        if (currentScore < bestFitness):\n",
    "          bestSolution, bestFitness = newX[i, :], currentScore\n",
    "\n",
    "    X = newX.copy()\n",
    "    random.shuffle(X)\n",
    "    X = np.clip(X, lb, ub)\n",
    "    self.population = np.round(np.asarray(X), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning and Optimization Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OWNER               = \"...\"     \n",
    "PBO                 = \"MRFO\"\n",
    "IS_DL               = True\n",
    "IS_MAX_PROBLEM      = True\n",
    "DATASET_PICKLE_FILE = \"PUT_YOUR_DATASET_PICKLE_FILE_NAME_HERE.p\"\n",
    "PICKELS_BASE_DIR    = \"...\" \n",
    "RESULTS_BASE_DIR    = \"...\" \n",
    "IMAGE_SHAPE         = (64, 64, 3)        \n",
    "CATEGORIES          = (\"NORMAL\", \"PNEUMONIA-Viral\", \"PNEUMONIA-Bacterial\", \"COVID-19\")\n",
    "ITERS               = 1000    \n",
    "EPOCHS              = 8      \n",
    "POPULATION_SIZE     = 20\n",
    "SKIP                = 0       \n",
    "POPULATION          = None    \n",
    "IS_NEW              = False   \n",
    "FF                  = FitnessFunction\n",
    "LOWER_BOUND         = 0.0\n",
    "UPPER_BOUND         = 1.0\n",
    "DIM                 = 4\n",
    "USE_TF              = True\n",
    "MODEL               = \"MobileNet\" \n",
    "TRAIN_RATIO         = 0.85    \n",
    "DROPOUT_RANGE       = [0, 6]\n",
    "OUTPUT_ACTIVATION   = \"softmax\"\n",
    "LOSS                = \"categorical_crossentropy\"\n",
    "PRETRAINED_WEIGHTS  = \"imagenet\"\n",
    "BATCH_SIZES         = list(np.arange(8, 100, 8))\n",
    "LEARN_RATIOS        = list(np.arange(0, 105, 5))\n",
    "REGULARIZERS        = []\n",
    "AUGMENTATION_CONFIG = {\n",
    "  \"rotation_range\"     : 15,\n",
    "  \"width_shift_range\"  : 0.15,\n",
    "  \"height_shift_range\" : 0.15,\n",
    "  \"shear_range\"        : 0.15,\n",
    "  \"zoom_range\"         : 0.15,\n",
    "  \"horizontal_flip\"    : True,\n",
    "  \"vertical_flip\"      : False,\n",
    "  \"brightness_range\"   : None,\n",
    "  \"fill_mode\"          : 'nearest'\n",
    "}\n",
    "OPTIMIZERS          = [\n",
    "  \"adam\", \"nadam\", \"adagrad\", \"adadelta\", \n",
    "  \"adamax\", \"rmsprop\", \"sgd\", \"ftrl\",\n",
    "]\n",
    "HIDDEN_ACTIVATIONS  = []\n",
    "WEIGHT_INITIALIZERS = []\n",
    "METRICS_LIST        = { \n",
    "  \"accuracy\"                                                              : True,         \n",
    "  f1_m                                                                    : True,                             \n",
    "  precision_m                                                             : True,                      \n",
    "  recall_m                                                                : True,                         \n",
    "  specificity_m                                                           : True,                    \n",
    "  AUC(name=\"hmb_auc\")                                                     : True,                            \n",
    "  sensitivity_m                                                           : True,                    \n",
    "  iou_coef_m                                                              : True,                       \n",
    "  dice_coef_m                                                             : True,                      \n",
    "  Precision(name=\"hmb_precision\")                                         : True,                      \n",
    "  Recall(name=\"hmb_recall\")                                               : True,                         \n",
    "  TruePositives(name=\"hmb_true_positive\")                                 : True,         \n",
    "  TrueNegatives(name=\"hmb_true_negative\")                                 : True,         \n",
    "  FalsePositives(name=\"hmb_false_positive\")                               : True,        \n",
    "  FalseNegatives(name=\"hmb_false_negative\")                               : True,        \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T19:43:48.030090Z",
     "start_time": "2021-02-09T19:43:48.016097Z"
    },
    "id": "g_QPtGMnLnkO"
   },
   "outputs": [],
   "source": [
    "RESULTS_DIR = os.path.join(BASE_DIR, RESULTS_BASE_DIR)\n",
    "DATASET_PICKLE = os.path.join(BASE_DIR, PICKELS_BASE_DIR, DATASET_PICKLE_FILE)\n",
    "\n",
    "OWNER_DIR = os.path.join(RESULTS_DIR, OWNER)\n",
    "if (not os.path.exists(OWNER_DIR)): os.mkdir(OWNER_DIR)\n",
    "\n",
    "HDF5_RESULTS_DIR = os.path.join(OWNER_DIR, f\"Models-{OWNER}\")\n",
    "FIGURES_RESULTS_DIR = os.path.join(OWNER_DIR, f\"Figures-{OWNER}\")\n",
    "HISTORY_RESULTS_DIR = os.path.join(OWNER_DIR, f\"History-{OWNER}\")\n",
    "STATE_PICKLE = os.path.join(OWNER_DIR, f\"State-{OWNER}.p\")\n",
    "LOGGER_FILE = os.path.join(OWNER_DIR, f\"Logger-{OWNER}.txt\")\n",
    "PATHS_FILE = os.path.join(OWNER_DIR, f\"Paths-{OWNER}.csv\")\n",
    "BEST_FILE = os.path.join(RESULTS_DIR, f\"Best-{OWNER}.csv\")\n",
    "\n",
    "SPLIT_DATASET_PICKLE = os.path.join(BASE_DIR, PICKELS_BASE_DIR,\n",
    "                  f\"Split-{DATASET_PICKLE_FILE}\")\n",
    "\n",
    "if (not os.path.exists(HDF5_RESULTS_DIR)): os.mkdir(HDF5_RESULTS_DIR)\n",
    "if (not os.path.exists(FIGURES_RESULTS_DIR)): os.mkdir(FIGURES_RESULTS_DIR)\n",
    "if (not os.path.exists(HISTORY_RESULTS_DIR)): os.mkdir(HISTORY_RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_KEYWORDS = []\n",
    "METRICS_NAMES = []\n",
    "METRICS = [d[0] for d in METRICS_LIST.items() if d[1]]\n",
    "if (IS_DL): \n",
    "  for m in METRICS:\n",
    "    if (\"tensorflow.python.keras.metrics\" in str(m)):\n",
    "      METRICS_NAMES.append(m.name)\n",
    "      nameList = m.name.strip().split(\"_\")[1:]\n",
    "      nameList = [s.capitalize() for s in nameList]\n",
    "      name = \" \".join(nameList)\n",
    "    elif (hasattr(m, '__call__')):\n",
    "      name = str(m).split()[1].strip()\n",
    "      METRICS_NAMES.append(name)\n",
    "      nameList = name.strip().split(\"_\")\n",
    "      nameList = [s.capitalize() for s in nameList]\n",
    "      name = \" \".join(nameList)\n",
    "    elif (isinstance(m, str)):\n",
    "      METRICS_NAMES.append(m)\n",
    "      name = m.strip().capitalize()\n",
    "    METRICS_KEYWORDS.append(name)\n",
    "  assert (len(METRICS) >= 6)\n",
    "  assert (len(METRICS) == len(METRICS_KEYWORDS) == len(METRICS_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T19:43:48.216982Z",
     "start_time": "2021-02-09T19:43:48.032091Z"
    },
    "id": "jVVgE8Kbc4w_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "currentState = HandleCurrentState(IS_NEW, STATE_PICKLE, LOGGER_FILE, DEBUG)\n",
    "\n",
    "if (IS_DL):\n",
    "  Q = LoadCreateSplitFile(currentState[\"DATASET_PICKLE\"],\n",
    "              currentState[\"SPLIT_DATASET_PICKLE\"],\n",
    "              len(currentState[\"CATEGORIES\"]),\n",
    "              currentState[\"TRAIN_RATIO\"], currentState[\"DEBUG\"])\n",
    "  [Xtrain, Xvalidation, Xtest, Ytrain, Yvalidation, Ytest] = Q\n",
    "\n",
    "  if (currentState[\"DEBUG\"]):\n",
    "    print(Xtrain.shape, Ytrain.shape)\n",
    "    print(Xtest.shape, Ytest.shape)\n",
    "    print(Xvalidation.shape, Yvalidation.shape)\n",
    "\n",
    "  LogEvent(currentState[\"LOGGER_FILE\"], \"=\" * 50)\n",
    "  LogEvent(currentState[\"LOGGER_FILE\"], Xtrain.shape)\n",
    "  LogEvent(currentState[\"LOGGER_FILE\"], Ytrain.shape)\n",
    "  LogEvent(currentState[\"LOGGER_FILE\"], Xvalidation.shape)\n",
    "  LogEvent(currentState[\"LOGGER_FILE\"], Yvalidation.shape)\n",
    "  LogEvent(currentState[\"LOGGER_FILE\"], Xtest.shape)\n",
    "  LogEvent(currentState[\"LOGGER_FILE\"], Ytest.shape)\n",
    "  LogEvent(currentState[\"LOGGER_FILE\"], \"=\" * 50)\n",
    "\n",
    "  PlotRandomFigure(Xtrain, Ytrain, currentState[\"CATEGORIES\"],\n",
    "           currentState[\"DEBUG\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PBO Checking and Object Creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T19:45:08.731101Z",
     "start_time": "2021-02-09T19:43:48.233973Z"
    },
    "id": "erfe6fhvc02J",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if (currentState[\"PBO\"] == \"MRFO\"):\n",
    "  pbo = MantaRayForagingOptimizer(FF, currentState)\n",
    "else:\n",
    "  print(\"Incorrect PBO Value.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (pbo): pbo.Run()\n",
    "else: print(\"Are you kidding?\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN and Population-based Optimization 2021-v10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "282.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 198.316666,
   "position": {
    "height": "40px",
    "left": "1096.53px",
    "right": "20px",
    "top": "120px",
    "width": "355.467px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
